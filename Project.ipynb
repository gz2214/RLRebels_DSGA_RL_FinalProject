{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 possible actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'] \n",
      "and 210 observations\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", )\n",
    "num_actions = env.action_space.n\n",
    "num_observations = env.observation_space.shape[0]  \n",
    "print(f\"There are {num_actions} possible actions: {env.unwrapped.get_action_meanings()} \\nand {num_observations} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rgb to grayscale\n",
    "def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to(device)\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        # block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        # block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, device):\n",
    "        self.env = env\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.device = device\n",
    "\n",
    "        # model setup\n",
    "        self.model = DQN(self.num_actions).to(device)\n",
    "        self.target_model = DQN(self.num_actions).to(device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # buffer and optimizer setup \n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.alpha = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_minimum = 0.05\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "                q_values = self.model(state)\n",
    "                return torch.argmax(q_values)  # Action with the highest Q-value\n",
    "            \n",
    "    def train(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return 0, 0\n",
    "        \n",
    "        samples = self.buffer.sample(batch_size)\n",
    "        state, action, reward, next_state, done = zip(*samples)\n",
    "\n",
    "        # Convert batches to tensors\n",
    "        batch_state = torch.tensor(batch_state, dtype=torch.float32, device=self.device)\n",
    "        batch_action = torch.tensor(batch_action, device=self.device)\n",
    "        batch_reward = torch.tensor(batch_reward, dtype=torch.float32, device=self.device)\n",
    "        batch_next_state = torch.tensor(batch_next_state, dtype=torch.float32, device=self.device)\n",
    "        batch_done = torch.tensor(batch_done, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Calculate current Q-values\n",
    "        q_values = self.model(batch_state)\n",
    "\n",
    "        # Calculate next Q-values from target model\n",
    "        next_q_values = self.target_model(next_state).max(1)[0].detach()\n",
    "        expected_q_values = batch_reward + (1 - done) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.loss_fn(q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_minimum)\n",
    "\n",
    "        return loss.item(), torch.max(q_values).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_minimum\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
